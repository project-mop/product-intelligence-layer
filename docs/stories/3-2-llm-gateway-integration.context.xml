<story-context id="3-2-llm-gateway-integration" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>2</storyId>
    <title>LLM Gateway Integration</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-27</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/3-2-llm-gateway-integration.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>system</asA>
    <iWant>to route intelligence API calls through an LLM gateway</iWant>
    <soThat>requests are processed by the appropriate AI model and return schema-constrained responses</soThat>
    <tasks>
      <task id="1">Define LLM Gateway Types and Interface (AC: 1)</task>
      <task id="2">Implement Anthropic Adapter (AC: 2, 8, 9)</task>
      <task id="3">Create Prompt Builder Service (AC: 3, 4)</task>
      <task id="4">Create Process Engine Service (AC: 3, 4, 5, 6, 7, 10)</task>
      <task id="5">Update Generate API Route (AC: 1-10)</task>
      <task id="6">Add Environment Variables (AC: 2, 8)</task>
      <task id="7">Write Unit Tests for LLM Gateway (AC: 1, 2, 5, 6, 7, 8, 9)</task>
      <task id="8">Write Unit Tests for Prompt Builder (AC: 3, 4)</task>
      <task id="9">Write Unit Tests for Process Engine (AC: 5, 6, 7, 10)</task>
      <task id="10">Write Integration Tests (AC: 1-10)</task>
      <task id="11">Verification (AC: 1-10)</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="1">LLM Gateway Interface: LLM Gateway interface defined in src/server/services/llm/types.ts with generate() method</criterion>
    <criterion id="2">Anthropic Adapter: Anthropic adapter implements LLMGateway interface at src/server/services/llm/anthropic.ts</criterion>
    <criterion id="3">Prompt Assembly: Prompt assembled from ProcessConfig (goal, input schema description, output schema description)</criterion>
    <criterion id="4">Input Data Handling: Input data passed to LLM as JSON in user message</criterion>
    <criterion id="5">JSON Response Parsing: LLM response parsed as JSON</criterion>
    <criterion id="6">Retry on Parse Failure: Parse failure triggers one retry with stricter prompt</criterion>
    <criterion id="7">Error on Second Failure: Second parse failure returns 500 with error code OUTPUT_PARSE_FAILED</criterion>
    <criterion id="8">Timeout Handling: LLM timeout (30s default) returns 503 with error code LLM_TIMEOUT</criterion>
    <criterion id="9">API Error Handling: Anthropic API errors return 503 with error code LLM_ERROR</criterion>
    <criterion id="10">Latency Tracking: Response includes meta.latency_ms measuring total request time</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-3.md</path>
        <title>Epic 3 Technical Specification</title>
        <section>Story 3.2: LLM Gateway Integration, Detailed Design</section>
        <snippet>LLM Gateway interface with generate() method, Anthropic adapter implementation, prompt assembly pattern, retry logic on parse failure.</snippet>
      </doc>
      <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Intelligence Generation Flow, LLM Gateway</section>
        <snippet>Provider-agnostic LLM Gateway pattern. Flow: Auth → Rate Limit → Input Validation → Cache → LLM → Output Validation → Response.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epic Breakdown</title>
        <section>Epic 3: API Generation &amp; Endpoints</section>
        <snippet>Story 3.2 routes intelligence API calls through LLM gateway. Constructs prompt from intelligence definition and input data.</snippet>
      </doc>
      <doc>
        <path>docs/testing-strategy-mvp.md</path>
        <title>Testing Strategy</title>
        <section>Test Types Overview, LLM Testing Strategy</section>
        <snippet>Unit tests with Vitest, mock @anthropic-ai/sdk to avoid real API calls. Integration tests use direct route handler calls.</snippet>
      </doc>
      <doc>
        <path>docs/stories/3-1-endpoint-url-generation.md</path>
        <title>Story 3.1 (Previous)</title>
        <section>Dev Agent Record</section>
        <snippet>Created API route structure, response helpers, API key validation. Returns 501 placeholder for LLM integration.</snippet>
      </doc>
    </docs>

    <code>
      <file>
        <path>src/app/api/v1/intelligence/[processId]/generate/route.ts</path>
        <kind>api-route</kind>
        <symbol>POST</symbol>
        <lines>1-133</lines>
        <reason>Main endpoint to update. Currently returns 501 placeholder. Replace with LLM generation logic.</reason>
      </file>
      <file>
        <path>src/server/services/api/response.ts</path>
        <kind>service</kind>
        <symbol>createSuccessResponse, createErrorResponse, ApiErrorCode</symbol>
        <lines>1-217</lines>
        <reason>Reuse for consistent API responses. Need to add LLM_TIMEOUT, LLM_ERROR, OUTPUT_PARSE_FAILED error codes.</reason>
      </file>
      <file>
        <path>src/server/services/process/types.ts</path>
        <kind>types</kind>
        <symbol>ProcessConfig, ComponentDefinition, AttributeDefinition</symbol>
        <lines>1-95</lines>
        <reason>Contains ProcessConfig with goal, systemPrompt, outputSchemaDescription needed for prompt assembly.</reason>
      </file>
      <file>
        <path>src/server/services/auth/api-key-validator.ts</path>
        <kind>service</kind>
        <symbol>validateApiKey, assertProcessAccess, ApiKeyContext</symbol>
        <lines>1-228</lines>
        <reason>Already integrated in generate route. Provides tenant context for process lookup.</reason>
      </file>
      <file>
        <path>src/lib/id.ts</path>
        <kind>utility</kind>
        <symbol>generateRequestId</symbol>
        <lines>87-89</lines>
        <reason>Generates req_* prefixed request IDs. Already used in generate route.</reason>
      </file>
      <file>
        <path>src/env.js</path>
        <kind>config</kind>
        <symbol>env</symbol>
        <lines>1-73</lines>
        <reason>Add ANTHROPIC_API_KEY, ANTHROPIC_MODEL, LLM_TIMEOUT_MS environment variables.</reason>
      </file>
      <file>
        <path>tests/integration/intelligence-api.test.ts</path>
        <kind>test</kind>
        <symbol>describe POST /api/v1/intelligence/:processId/generate</symbol>
        <lines>1-150+</lines>
        <reason>18 existing tests for auth flow. Extend with LLM generation tests using mock gateway.</reason>
      </file>
    </code>

    <dependencies>
      <node>
        <package name="@anthropic-ai/sdk" version="^0.35.x" status="to-install">Official Anthropic Claude SDK for LLM calls</package>
        <package name="next" version="^15.2.3" status="installed">Next.js framework</package>
        <package name="zod" version="^3.24.2" status="installed">Schema validation</package>
        <package name="vitest" version="^4.0.14" status="installed">Testing framework</package>
        <package name="@tanstack/react-query" version="^5.69.0" status="installed">Data fetching</package>
        <package name="prisma" version="^7.0.1" status="installed">Database ORM</package>
      </node>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="pattern">Provider-agnostic LLM Gateway pattern (ADR-002): Interface in types.ts, adapter in anthropic.ts</constraint>
    <constraint type="pattern">REST API for public endpoints (ADR-003): Not tRPC, use Next.js route handlers</constraint>
    <constraint type="response-format">Success: { success: true, data: {...}, meta: { version, cached, latency_ms, request_id } }</constraint>
    <constraint type="response-format">Error: { success: false, error: { code, message } }</constraint>
    <constraint type="error-codes">New codes needed: LLM_TIMEOUT (503), LLM_ERROR (503), OUTPUT_PARSE_FAILED (500)</constraint>
    <constraint type="testing">Mock @anthropic-ai/sdk in unit tests - never hit live API in CI</constraint>
    <constraint type="testing">Use dependency injection to allow gateway mocking in integration tests</constraint>
    <constraint type="performance">P95 response time &lt; 2 seconds, P99 &lt; 5 seconds, LLM timeout 30 seconds</constraint>
    <constraint type="security">Never expose compiled prompts to customers (FR-801)</constraint>
    <constraint type="retry">One retry on JSON parse failure with stricter prompt before returning 500</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>LLMGateway</name>
      <kind>interface</kind>
      <signature>interface LLMGateway { generate(params: GenerateParams): Promise&lt;GenerateResult&gt;; }</signature>
      <path>src/server/services/llm/types.ts (to create)</path>
    </interface>
    <interface>
      <name>GenerateParams</name>
      <kind>type</kind>
      <signature>{ prompt: string; systemPrompt?: string; maxTokens: number; temperature: number; model?: string; }</signature>
      <path>src/server/services/llm/types.ts (to create)</path>
    </interface>
    <interface>
      <name>GenerateResult</name>
      <kind>type</kind>
      <signature>{ text: string; usage: { inputTokens: number; outputTokens: number; }; model: string; durationMs: number; }</signature>
      <path>src/server/services/llm/types.ts (to create)</path>
    </interface>
    <interface>
      <name>assemblePrompt</name>
      <kind>function</kind>
      <signature>function assemblePrompt(processVersion: ProcessVersion, input: Record&lt;string, unknown&gt;): { system: string; user: string; }</signature>
      <path>src/server/services/process/prompt.ts (to create)</path>
    </interface>
    <interface>
      <name>ProcessEngine.generateIntelligence</name>
      <kind>method</kind>
      <signature>generateIntelligence(processVersion: ProcessVersion, input: Record&lt;string, unknown&gt;): Promise&lt;IntelligenceResult&gt;</signature>
      <path>src/server/services/process/engine.ts (to create)</path>
    </interface>
    <interface>
      <name>POST /api/v1/intelligence/:processId/generate</name>
      <kind>REST endpoint</kind>
      <signature>POST, Bearer auth, Body: { input: Record&lt;string, unknown&gt; }, Response: SuccessResponse or ErrorResponse</signature>
      <path>src/app/api/v1/intelligence/[processId]/generate/route.ts</path>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Unit tests use Vitest with mocked dependencies. Integration tests use Vitest with direct route handler calls and real database.
      Mock @anthropic-ai/sdk to avoid real API calls and costs. Use VCR pattern or recorded responses.
      50% coverage minimum for MVP. Test framework locations: tests/unit/, tests/integration/.
    </standards>
    <locations>
      <location>tests/unit/llm-gateway.test.ts (to create)</location>
      <location>tests/unit/prompt-builder.test.ts (to create)</location>
      <location>tests/unit/process-engine.test.ts (to create)</location>
      <location>tests/integration/intelligence-api.test.ts (extend)</location>
    </locations>
    <ideas>
      <idea ac="1,2">Test AnthropicGateway initialization with and without API key</idea>
      <idea ac="2,8">Test timeout handling - mock delayed response exceeding LLM_TIMEOUT_MS</idea>
      <idea ac="2,9">Test API error handling - mock 429 rate limit, 500 server errors</idea>
      <idea ac="3,4">Test prompt assembly includes goal, outputSchemaDescription, JSON-stringified input</idea>
      <idea ac="5">Test successful JSON parsing of LLM response</idea>
      <idea ac="6">Test retry logic - first parse failure triggers retry with stricter prompt</idea>
      <idea ac="7">Test error after second parse failure returns 500 OUTPUT_PARSE_FAILED</idea>
      <idea ac="10">Test latency_ms is present and accurate in response meta</idea>
      <idea ac="1-10">Integration test: Full generation flow with mocked gateway</idea>
      <idea ac="1-10">Integration test: 503 response structure on LLM timeout/error</idea>
    </ideas>
  </tests>
</story-context>
